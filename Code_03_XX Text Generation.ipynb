{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88015722",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "#Set to avoid warning messages.\n",
    "transformers.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ac3d23",
   "metadata": {},
   "source": [
    "## 03.02. Content Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e4567c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/olympiasaha/anaconda3/envs/Transformers2/lib/python3.9/site-packages/transformers/generation/utils.py:1219: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural Language Processing is a growing domain in machine learning applications. The purpose of this project is to provide a scalable way to develop a variety of languages on top of neural net technologies. Some applications are being developed that use the underlying neural network and can easily integrate with existing applications. These are shown below demonstrating \n",
      "-----------------\n",
      "Natural Language Processing is a growing domain in machine learning and there are numerous applications available in the field of AI in general. With the current state of AI research in AI, we are exploring these topics, including:\n",
      "\n",
      "Using AI for Machine Learning Science/Sustainability\n",
      "\n",
      "Using Machine Learning in \n",
      "-----------------\n",
      "Natural Language Processing is a growing domain in machine learning. This means that every new task, even one you've never thought to create, may be used as a foundation for generating a more comprehensive set of AI-based machine learning algorithms. But the problem of human error is getting the attention of other researchers \n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "text_generator = pipeline(\"text-generation\", \n",
    "                          model=\"gpt2\")\n",
    "transformers.set_seed(1)\n",
    "\n",
    "input_text=\"Natural Language Processing is a \\\n",
    "growing domain in machine learning\"\n",
    "\n",
    "synthetic_text=text_generator(input_text,\n",
    "                              num_return_sequences=3,\n",
    "                              max_new_tokens=50)\n",
    "\n",
    "for text in synthetic_text:\n",
    "    print(text.get(\"generated_text\") ,\"\\n-----------------\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863bfa0e",
   "metadata": {},
   "source": [
    "## 03.04. Chatbot Conversation Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10b9b6db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 1.51k/1.51k [00:00<00:00, 748kB/s]\n",
      "Downloading pytorch_model.bin: 100%|█████████████████| 350M/350M [00:23<00:00, 14.7MB/s]\n",
      "Downloading (…)neration_config.json: 100%|██████████████| 311/311 [00:00<00:00, 137kB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|█████████████| 205/205 [00:00<00:00, 86.3kB/s]\n",
      "Downloading (…)olve/main/vocab.json: 100%|███████████| 964k/964k [00:00<00:00, 3.39MB/s]\n",
      "Downloading (…)olve/main/merges.txt: 100%|███████████| 345k/345k [00:00<00:00, 2.91MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|███████████| 99.0/99.0 [00:00<00:00, 39.6kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BlenderbotSmallConfig {\n",
      "  \"_name_or_path\": \"facebook/blenderbot_small-90M\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BlenderbotSmallForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 8,\n",
      "  \"decoder_start_token_id\": 1,\n",
      "  \"do_blenderbot_90_layernorm\": true,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 8,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layernorm_variant\": \"xlm\",\n",
      "  \"length_penalty\": 0.65,\n",
      "  \"max_length\": 128,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"min_length\": 20,\n",
      "  \"model_type\": \"blenderbot-small\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 10,\n",
      "  \"num_hidden_layers\": 8,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"unk_token_id\": 3,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 54944\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import  Conversation\n",
    "\n",
    "conversational_pipeline = pipeline(\"conversational\", \n",
    "                                   model=\"facebook/blenderbot_small-90M\")\n",
    "\n",
    "print(conversational_pipeline.model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "943e7009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First Exchange: \n",
      "--------------------\n",
      " User Input: Do you have any hobbies?\n",
      " Bot Output: yes, i love going to the beach. what about you? do you have any hobbies?\n",
      "\n",
      "Second Exchange: \n",
      "--------------------\n",
      " User Input: I like to watch movies\n",
      " Bot Output: i love going to the beach. i also like to watch movies. what kind of movies do you like?\n",
      "\n",
      "Third Exchange: \n",
      "--------------------\n",
      " User Input: action movies\n",
      " Bot Output: i love going to the beach. i also like to watch movies. what kind of movies do you like?\n",
      "\n",
      "Accessing All Responses: \n",
      "Conversation id: c6a0d092-edb9-4a30-8587-9592cc56005f \n",
      "user >> Do you have any hobbies? \n",
      "bot >> yes, i love going to the beach. what about you? do you have any hobbies? \n",
      "user >> I like to watch movies \n",
      "bot >> i love going to the beach. i also like to watch movies. what kind of movies do you like? \n",
      "user >> action movies \n",
      "bot >> i love going to the beach as well. i like action movies as well, but i don't get to see them often. what's your favorite action movie? \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Sample inputs\n",
    "first_input=\"Do you have any hobbies?\"\n",
    "second_input = \"I like to watch movies\"\n",
    "third_input = \"action movies\"\n",
    "\n",
    "#Create a context\n",
    "bot_conversation = Conversation(first_input)\n",
    "\n",
    "print(\"\\nFirst Exchange: \\n--------------------\")\n",
    "\n",
    "conversational_pipeline(bot_conversation)\n",
    "print(\" User Input:\", bot_conversation.past_user_inputs[0])\n",
    "print(\" Bot Output:\", bot_conversation.generated_responses[0])\n",
    "\n",
    "print(\"\\nSecond Exchange: \\n--------------------\")\n",
    "bot_conversation.add_user_input(second_input)\n",
    "conversational_pipeline(bot_conversation)\n",
    "\n",
    "print(\" User Input:\", bot_conversation.past_user_inputs[1])\n",
    "print(\" Bot Output:\", bot_conversation.generated_responses[1])\n",
    "\n",
    "print(\"\\nThird Exchange: \\n--------------------\")\n",
    "bot_conversation.add_user_input(third_input)\n",
    "conversational_pipeline(bot_conversation)\n",
    "\n",
    "print(\" User Input:\", bot_conversation.past_user_inputs[2])\n",
    "print(\" Bot Output:\", bot_conversation.generated_responses[1])\n",
    "\n",
    "print(\"\\nAccessing All Responses: \")\n",
    "print(bot_conversation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf9be7f",
   "metadata": {},
   "source": [
    "## 03.06. Translating with Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0ea71b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 1.21k/1.21k [00:00<00:00, 456kB/s]\n",
      "Downloading pytorch_model.bin: 100%|█████████████████| 892M/892M [00:53<00:00, 16.7MB/s]\n",
      "Downloading (…)neration_config.json: 100%|█████████████| 147/147 [00:00<00:00, 31.0kB/s]\n",
      "Downloading (…)ve/main/spiece.model: 100%|███████████| 792k/792k [00:00<00:00, 11.4MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|█████████| 1.39M/1.39M [00:00<00:00, 3.14MB/s]\n",
      "/Users/olympiasaha/anaconda3/envs/Transformers2/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "German Translation:  Acme ist ein Technologieunternehmen mit Sitz in New York und Paris.\n",
      "French Translation:  Acme est une société technologique basée à New York et à Paris.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
    "\n",
    "source_english=\"Acme is a technology company based in New York and Paris\"\n",
    "\n",
    "inputs_german = tokenizer(\n",
    "    \"translate English to German: \" + source_english,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "outputs_german = model.generate(\n",
    "    inputs_german[\"input_ids\"], \n",
    "    max_length=40)\n",
    "\n",
    "print(\"German Translation: \",\n",
    "      tokenizer.decode(outputs_german[0], \n",
    "                       skip_special_tokens=True))\n",
    "\n",
    "inputs_french = tokenizer(\n",
    "    \"translate English to French: \" + source_english, \n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "outputs_french = model.generate(\n",
    "    inputs_french[\"input_ids\"], \n",
    "    max_length=40)\n",
    "\n",
    "print(\"French Translation: \", \n",
    "      tokenizer.decode(outputs_french[0], \n",
    "                       skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b4eacb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
